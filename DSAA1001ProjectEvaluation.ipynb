{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-KRYDZmkJOb"
      },
      "source": [
        "In this project, you will be tasked with predicting the 'Status' of flights based on historical flight data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aaqs8YZEkXNd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=10d1zrylGKQQp_Sl-5ejuphn4Q1vKDIo8\n",
            "From (redirected): https://drive.google.com/uc?id=10d1zrylGKQQp_Sl-5ejuphn4Q1vKDIo8&confirm=t&uuid=471200df-2839-47e4-9518-264d5535e164\n",
            "To: /Users/yushan/workplace/DSAD1001/flights-data.tgz\n",
            "100%|██████████| 32.9M/32.9M [00:17<00:00, 1.92MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x flights-data/2016_10.csv\n",
            "x flights-data/2016_11.csv\n",
            "x flights-data/2016_12.csv\n",
            "x flights-data/2016_1.csv\n",
            "x flights-data/2016_2.csv\n",
            "x flights-data/2016_3.csv\n",
            "x flights-data/2016_4.csv\n",
            "x flights-data/2016_5.csv"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "x flights-data/2016_6.csv\n",
            "x flights-data/2016_7.csv\n",
            "x flights-data/2016_8.csv\n",
            "x flights-data/2016_9.csv\n",
            "x flights-data/2017_10.csv\n",
            "x flights-data/2017_1.csv\n",
            "x flights-data/2017_2.csv\n",
            "x flights-data/2017_3.csv\n",
            "x flights-data/2017_4.csv\n",
            "x flights-data/2017_5.csv\n",
            "x flights-data/2017_6.csv\n",
            "x flights-data/2017_7.csv\n",
            "x flights-data/2017_8.csv\n",
            "x flights-data/2017_9.csv\n"
          ]
        }
      ],
      "source": [
        "# You might use this block to download the training data\n",
        "import os\n",
        "import gdown\n",
        "\n",
        "if not os.path.exists(\"flights-data.tgz\"):\n",
        "    data_url = \"https://drive.google.com/file/d/10d1zrylGKQQp_Sl-5ejuphn4Q1vKDIo8/view?usp=share_link\"\n",
        "    gdown.download(data_url, fuzzy=True)\n",
        "    !tar -xvzf \"flights-data.tgz\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFPayuRGlFGS"
      },
      "source": [
        "Now, you need to implement your training code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Sx8wFHIilL4k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading training data...\n",
            "\n",
            "Training with 5-fold cross validation...\n",
            "Fold 1 F1-score (macro): 0.3671\n",
            "Fold 2 F1-score (macro): 0.3671\n",
            "Fold 3 F1-score (macro): 0.3665\n",
            "Fold 4 F1-score (macro): 0.3662\n",
            "Fold 5 F1-score (macro): 0.3661\n",
            "\n",
            "Average F1-score (macro): 0.3666\n",
            "\n",
            "Predicting test data...\n",
            "Prediction completed and saved!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 学生信息\n",
        "student_id = \"50012962\"\n",
        "student_name = \"Bowen_LIU\"\n",
        "\n",
        "# 读取训练数据\n",
        "def load_train_data():\n",
        "    all_files = glob.glob(\"flights-data/*.csv\")\n",
        "    df_list = []\n",
        "    for file in all_files:\n",
        "        df = pd.read_csv(file)\n",
        "        df_list.append(df)\n",
        "    return pd.concat(df_list, axis=0, ignore_index=True)\n",
        "\n",
        "# 数据预处理\n",
        "def preprocess_data(df):\n",
        "    # 选择有用特征\n",
        "    features = ['Quarter', 'Month', 'DayOfWeek', 'UniqueCarrier', 'Origin', \n",
        "                'Dest', 'CRSDepTime', 'CRSArrTime', 'Distance', 'DistanceGroup']\n",
        "    \n",
        "    df_processed = df[features].copy()\n",
        "    \n",
        "    # 处理分类变量\n",
        "    le_dict = {}\n",
        "    for col in ['UniqueCarrier', 'Origin', 'Dest']:\n",
        "        le = LabelEncoder()\n",
        "        df_processed[col] = le.fit_transform(df_processed[col])\n",
        "        le_dict[col] = le\n",
        "        \n",
        "    # 时间特征处理\n",
        "    df_processed['CRSDepTime'] = df_processed['CRSDepTime'].apply(lambda x: int(str(int(x)).zfill(4)[:2]))\n",
        "    df_processed['CRSArrTime'] = df_processed['CRSArrTime'].apply(lambda x: int(str(int(x)).zfill(4)[:2]))\n",
        "    \n",
        "    return df_processed, le_dict\n",
        "\n",
        "# 主程序\n",
        "def main():\n",
        "    # 加载训练数据\n",
        "    print(\"Loading training data...\")\n",
        "    train_data = load_train_data()\n",
        "    X_train, le_dict = preprocess_data(train_data)\n",
        "    y_train = train_data['Status']\n",
        "    \n",
        "    # 5折交叉验证\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    f1_scores = []\n",
        "    \n",
        "    print(\"\\nTraining with 5-fold cross validation...\")\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "        X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "        \n",
        "        rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "        rf.fit(X_fold_train, y_fold_train)\n",
        "        \n",
        "        y_pred = rf.predict(X_fold_val)\n",
        "        f1 = f1_score(y_fold_val, y_pred, average='macro')\n",
        "        f1_scores.append(f1)\n",
        "        print(f\"Fold {fold+1} F1-score (macro): {f1:.4f}\")\n",
        "    \n",
        "    print(f\"\\nAverage F1-score (macro): {np.mean(f1_scores):.4f}\")\n",
        "    \n",
        "    # 加载测试数据并预测\n",
        "    print(\"\\nPredicting test data...\")\n",
        "    if not os.path.exists(\"test_data.csv\"):\n",
        "        data_url = \"https://drive.google.com/file/d/1eLZRSP9zb9KkdeRg-8CGuak3a_xLS6YF/view?usp=drive_link\"\n",
        "        gdown.download(data_url, fuzzy=True)\n",
        "    \n",
        "    test_data = pd.read_csv(\"test_data.csv\")\n",
        "    X_test, _ = preprocess_data(test_data)\n",
        "    \n",
        "    # 使用全量训练数据训练最终模型\n",
        "    final_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    final_model.fit(X_train, y_train)\n",
        "    \n",
        "    test_data['Status'] = final_model.predict(X_test)\n",
        "    test_data.to_csv(f\"{student_id}_{student_name}.csv\", index=False)\n",
        "    print(\"Prediction completed and saved!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOS11dATlOow"
      },
      "source": [
        "Then, it's time to predict and submit the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "使用设备：cpu\n",
            "开始读取CSV文件...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "读取CSV文件: 100%|██████████| 22/22 [00:01<00:00, 17.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "成功读取22个CSV文件，正在合并数据...\n",
            "数据合并完成，共1034931行，36列。\n",
            "开始处理缺失值...\n",
            "缺失值处理完成，删除了2499行，剩余1032432行。\n",
            "开始编码分类变量...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "编码分类变量: 100%|██████████| 5/5 [00:00<00:00, 17.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "编码目标变量Status...\n",
            "开始5折交叉验证训练...\n",
            "\n",
            "Fold 1训练中...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 Epoch [1/5], Loss: 0.5666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 Epoch [2/5], Loss: 0.5178\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 Epoch [3/5], Loss: 0.5174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 Epoch [4/5], Loss: 0.5170\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 Epoch [5/5], Loss: 0.5168\n",
            "Fold 1 验证集 F1 分数: 0.2981, 准确率: 0.8086\n",
            "\n",
            "Fold 2训练中...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2 Epoch [1/5], Loss: 0.5249\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2 Epoch [2/5], Loss: 0.5177\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2 Epoch [3/5], Loss: 0.5173\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2 Epoch [4/5], Loss: 0.5170\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2 Epoch [5/5], Loss: 0.5167\n",
            "Fold 2 验证集 F1 分数: 0.2981, 准确率: 0.8086\n",
            "\n",
            "Fold 3训练中...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3 Epoch [1/5], Loss: 0.5242\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3 Epoch [2/5], Loss: 0.5175\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3 Epoch [3/5], Loss: 0.5172\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3 Epoch [4/5], Loss: 0.5169\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3 Epoch [5/5], Loss: 0.5167\n",
            "Fold 3 验证集 F1 分数: 0.2981, 准确率: 0.8086\n",
            "\n",
            "Fold 4训练中...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 4 Epoch [1/5], Loss: 0.5233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 4 Epoch [2/5], Loss: 0.5175\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 4 Epoch [3/5], Loss: 0.5172\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 4 Epoch [4/5], Loss: 0.5169\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 4 Epoch [5/5], Loss: 0.5166\n",
            "Fold 4 验证集 F1 分数: 0.2981, 准确率: 0.8086\n",
            "\n",
            "Fold 5训练中...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 5 Epoch [1/5], Loss: 0.5257\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 5 Epoch [2/5], Loss: 0.5176\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 5 Epoch [3/5], Loss: 0.5174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 5 Epoch [4/5], Loss: 0.5170\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 5 Epoch [5/5], Loss: 0.5169\n",
            "Fold 5 验证集 F1 分数: 0.2981, 准确率: 0.8086\n",
            "\n",
            "平均 F1 分数: 0.2981\n",
            "平均 准确率: 0.8086\n",
            "\n",
            "开始处理测试数据...\n",
            "测试数据共94379行，35列。\n",
            "处理缺失值后，测试数据剩余94133行。\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "编码测试数据分类变量: 100%|██████████| 5/5 [00:00<00:00, 222.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "开始预测测试数据...\n",
            "预测结果已保存至 prediction_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 导入必要的库\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 检查是否可使用MPS加速\n",
        "device = torch.device(\"cpu\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"使用设备：{device}\")\n",
        "\n",
        "# 设置学生信息\n",
        "student_name = 'Your Name'\n",
        "student_id = 'Your Student ID'\n",
        "\n",
        "# 1. 读取并合并2016-2017年的航班数据\n",
        "csv_files = glob.glob('flights-data/*.csv')\n",
        "data_list = []\n",
        "print(\"开始读取CSV文件...\")\n",
        "for file in tqdm(csv_files, desc=\"读取CSV文件\"):\n",
        "    df = pd.read_csv(file)\n",
        "    data_list.append(df)\n",
        "print(f\"成功读取{len(csv_files)}个CSV文件，正在合并数据...\")\n",
        "data = pd.concat(data_list, ignore_index=True)\n",
        "print(f\"数据合并完成，共{data.shape[0]}行，{data.shape[1]}列。\")\n",
        "\n",
        "# 2. 处理缺失值\n",
        "print(\"开始处理缺失值...\")\n",
        "initial_rows = data.shape[0]\n",
        "data.dropna(inplace=True)\n",
        "removed_rows = initial_rows - data.shape[0]\n",
        "print(f\"缺失值处理完成，删除了{removed_rows}行，剩余{data.shape[0]}行。\")\n",
        "\n",
        "# 3. 编码分类变量\n",
        "print(\"开始编码分类变量...\")\n",
        "categorical_features = ['UniqueCarrier', 'Origin', 'Dest', 'DepTimeBlk', 'ArrTimeBlk']\n",
        "label_encoders = {}\n",
        "for col in tqdm(categorical_features, desc=\"编码分类变量\"):\n",
        "    le = LabelEncoder()\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# 4. 处理时间特征（无需额外处理，已是数值型）\n",
        "time_features = ['Quarter', 'Month', 'DayofMonth', 'DayOfWeek']\n",
        "\n",
        "# 5. 选择有效特征\n",
        "features = time_features + categorical_features + ['Flights', 'Distance', 'DistanceGroup']\n",
        "X = data[features]\n",
        "y = data['Status']\n",
        "\n",
        "# 将目标变量编码\n",
        "print(\"编码目标变量Status...\")\n",
        "status_le = LabelEncoder()\n",
        "y = status_le.fit_transform(y)\n",
        "\n",
        "# 转换为NumPy数组\n",
        "X = X.values\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "# 转换为Tensor并移动到设备上\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long).to(device)\n",
        "\n",
        "# 6. 定义神经网络模型\n",
        "class FlightDelayModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(FlightDelayModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(32, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "# 参数设置\n",
        "input_size = X.shape[1]\n",
        "num_classes = len(np.unique(y))\n",
        "num_epochs = 5  # 为了加快示例运行次数，可根据需要增加\n",
        "batch_size = 1024\n",
        "learning_rate = 0.001\n",
        "\n",
        "# 定义模型、损失函数和优化器\n",
        "model = FlightDelayModel(input_size, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 7. 进行5折交叉验证\n",
        "print(\"开始5折交叉验证训练...\")\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold = 1\n",
        "f1_scores = []\n",
        "accuracies = []\n",
        "\n",
        "for train_index, val_index in skf.split(X, y):\n",
        "    print(f\"\\nFold {fold}训练中...\")\n",
        "    X_train, X_val = X_tensor[train_index], X_tensor[val_index]\n",
        "    y_train, y_val = y_tensor[train_index], y_tensor[val_index]\n",
        "    \n",
        "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "    val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
        "    \n",
        "    # 重置模型参数\n",
        "    def reset_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            m.reset_parameters()\n",
        "    model.apply(reset_weights)\n",
        "    \n",
        "    # 训练\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for inputs, labels in tqdm(train_loader, desc=f\"Fold {fold} Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.view(-1).to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"Fold {fold} Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    # 验证\n",
        "    model.eval()\n",
        "    y_val_pred = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            y_val_pred.extend(predicted.cpu().numpy())\n",
        "    y_val_cpu = y_val.cpu().numpy()\n",
        "    f1 = f1_score(y_val_cpu, y_val_pred, average='macro')\n",
        "    acc = accuracy_score(y_val_cpu, y_val_pred)\n",
        "    f1_scores.append(f1)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"Fold {fold} 验证集 F1 分数: {f1:.4f}, 准确率: {acc:.4f}\")\n",
        "    fold += 1\n",
        "\n",
        "# 输出平均F1分数和准确率\n",
        "print(f\"\\n平均 F1 分数: {np.mean(f1_scores):.4f}\")\n",
        "print(f\"平均 准确率: {np.mean(accuracies):.4f}\")\n",
        "\n",
        "# 8. 特征重要性分析（由于神经网络无法直接获取特征重要性，可使用Permuation Importance等方法）\n",
        "# 此处仅作为示例，不具体实现\n",
        "\n",
        "# 9. 对测试数据进行同样的预处理\n",
        "print(\"\\n开始处理测试数据...\")\n",
        "test_data = pd.read_csv('test_data.csv')\n",
        "print(f\"测试数据共{test_data.shape[0]}行，{test_data.shape[1]}列。\")\n",
        "test_data.dropna(inplace=True)\n",
        "print(f\"处理缺失值后，测试数据剩余{test_data.shape[0]}行。\")\n",
        "\n",
        "# 编码测试数据的分类变量\n",
        "for col in tqdm(categorical_features, desc=\"编码测试数据分类变量\"):\n",
        "    le = label_encoders[col]\n",
        "    test_data[col] = le.transform(test_data[col])\n",
        "\n",
        "# 提取特征并转换为Tensor\n",
        "X_test = test_data[features].values\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "\n",
        "# 预测测试数据的Status\n",
        "print(\"开始预测测试数据...\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, y_test_pred = torch.max(outputs.data, 1)\n",
        "y_test_pred_labels = status_le.inverse_transform(y_test_pred.cpu().numpy())\n",
        "\n",
        "# 将预测结果保存为规定格式的csv文件\n",
        "test_data['Status'] = y_test_pred_labels\n",
        "test_data[['Status']].to_csv('prediction_results.csv', index=False)\n",
        "print(\"预测结果已保存至 prediction_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "开始读取CSV文件...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "读取CSV文件: 100%|██████████| 22/22 [00:01<00:00, 17.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "成功读取22个CSV文件，正在合并数据...\n",
            "数据合并完成，共1034931行，36列。\n",
            "开始处理缺失值...\n",
            "缺失值处理完成，删除了2499行，剩余1032432行。\n",
            "Status列的唯一值： ['normal' 'delay' 'cancel']\n",
            "开始编码分类变量...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "编码分类变量: 100%|██████████| 5/5 [00:00<00:00, 16.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "拆分训练集和测试集...\n",
            "训练集大小：825945，验证集大小：206487\n",
            "开始模型训练和调优...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "python(23943) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(23944) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(23945) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(23946) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(23947) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(23948) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(23949) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(23950) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(23951) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(23952) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m cv \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     88\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mrfc, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39mcv, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# 输出最佳参数\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m最佳参数：\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAA1001FP/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAA1001FP/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1023\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1018\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1023\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAA1001FP/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1570\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1570\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAA1001FP/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    965\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    966\u001b[0m         )\n\u001b[1;32m    967\u001b[0m     )\n\u001b[0;32m--> 969\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m     )\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAA1001FP/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAA1001FP/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAA1001FP/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAA1001FP/lib/python3.9/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 导入必要的库\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 设置学生信息\n",
        "student_name = 'Your Name'\n",
        "student_id = 'Your Student ID'\n",
        "\n",
        "# 1. 读取并合并2016-2017年的航班数据\n",
        "csv_files = glob.glob('flights-data/*.csv')\n",
        "data_list = []\n",
        "print(\"开始读取CSV文件...\")\n",
        "for file in tqdm(csv_files, desc=\"读取CSV文件\"):\n",
        "    df = pd.read_csv(file)\n",
        "    data_list.append(df)\n",
        "print(f\"成功读取{len(csv_files)}个CSV文件，正在合并数据...\")\n",
        "data = pd.concat(data_list, ignore_index=True)\n",
        "print(f\"数据合并完成，共{data.shape[0]}行，{data.shape[1]}列。\")\n",
        "\n",
        "# 2. 数据清洗和预处理\n",
        "print(\"开始处理缺失值...\")\n",
        "initial_rows = data.shape[0]\n",
        "data.dropna(inplace=True)\n",
        "removed_rows = initial_rows - data.shape[0]\n",
        "print(f\"缺失值处理完成，删除了{removed_rows}行，剩余{data.shape[0]}行。\")\n",
        "\n",
        "# 确认Status列的取值\n",
        "print(\"Status列的唯一值：\", data['Status'].unique())\n",
        "\n",
        "# 将Status列编码为数字\n",
        "status_le = LabelEncoder()\n",
        "data['Status'] = status_le.fit_transform(data['Status'])\n",
        "\n",
        "# 3. 编码分类变量\n",
        "print(\"开始编码分类变量...\")\n",
        "categorical_features = ['UniqueCarrier', 'Origin', 'Dest', 'DepTimeBlk', 'ArrTimeBlk']\n",
        "label_encoders = {}\n",
        "for col in tqdm(categorical_features, desc=\"编码分类变量\"):\n",
        "    le = LabelEncoder()\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# 4. 处理时间特征\n",
        "# CRSDepTime和CRSArrTime可能需要处理为小时或分钟数\n",
        "def convert_time(x):\n",
        "    x = int(x)\n",
        "    if x == 2400:\n",
        "        x = 0\n",
        "    hours = x // 100\n",
        "    minutes = x % 100\n",
        "    return hours * 60 + minutes\n",
        "\n",
        "data['CRSDepTime'] = data['CRSDepTime'].apply(convert_time)\n",
        "data['CRSArrTime'] = data['CRSArrTime'].apply(convert_time)\n",
        "\n",
        "# 5. 选择特征和目标变量\n",
        "features = ['Quarter', 'Month', 'DayofMonth', 'DayOfWeek',\n",
        "            'CRSDepTime', 'CRSArrTime', 'UniqueCarrier', 'Origin', 'Dest',\n",
        "            'DepTimeBlk', 'ArrTimeBlk', 'Distance', 'DistanceGroup']\n",
        "\n",
        "X = data[features]\n",
        "y = data['Status']\n",
        "\n",
        "# 6. 拆分训练集和测试集\n",
        "print(\"拆分训练集和测试集...\")\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "print(f\"训练集大小：{X_train.shape[0]}，验证集大小：{X_valid.shape[0]}\")\n",
        "\n",
        "# 7. 使用随机森林进行模型训练\n",
        "print(\"开始模型训练和调优...\")\n",
        "rfc = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 定义参数网格\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "# 进行5折交叉验证\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=cv, scoring='f1_macro', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 输出最佳参数\n",
        "print(\"最佳参数：\", grid_search.best_params_)\n",
        "\n",
        "# 在验证集上预测\n",
        "print(\"在验证集上进行预测...\")\n",
        "best_rfc = grid_search.best_estimator_\n",
        "y_pred = best_rfc.predict(X_valid)\n",
        "\n",
        "# 输出F1分数和准确率\n",
        "f1 = f1_score(y_valid, y_pred, average='macro')\n",
        "acc = accuracy_score(y_valid, y_pred)\n",
        "print(f\"验证集 F1 分数: {f1:.4f}\")\n",
        "print(f\"验证集 准确率: {acc:.4f}\")\n",
        "\n",
        "# 混淆矩阵和分类报告\n",
        "print(\"分类报告：\")\n",
        "print(classification_report(y_valid, y_pred, target_names=status_le.classes_))\n",
        "\n",
        "# 8. 特征重要性分析\n",
        "importances = best_rfc.feature_importances_\n",
        "feature_importance = pd.Series(importances, index=features).sort_values(ascending=False)\n",
        "print(\"特征重要性：\")\n",
        "print(feature_importance)\n",
        "\n",
        "# 可视化特征重要性\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,6))\n",
        "feature_importance.plot(kind='bar')\n",
        "plt.title('Feature Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 9. 对测试数据进行同样的预处理，并进行预测\n",
        "print(\"开始处理测试数据并进行预测...\")\n",
        "test_data = pd.read_csv('test_data.csv')\n",
        "print(f\"测试数据共{test_data.shape[0]}行，{test_data.shape[1]}列。\")\n",
        "test_data.dropna(inplace=True)\n",
        "print(f\"处理缺失值后，测试数据剩余{test_data.shape[0]}行。\")\n",
        "\n",
        "# 编码测试数据的分类变量\n",
        "for col in categorical_features:\n",
        "    le = label_encoders[col]\n",
        "    test_data[col] = le.transform(test_data[col])\n",
        "\n",
        "# 处理时间特征\n",
        "test_data['CRSDepTime'] = test_data['CRSDepTime'].apply(convert_time)\n",
        "test_data['CRSArrTime'] = test_data['CRSArrTime'].apply(convert_time)\n",
        "\n",
        "# 提取特征\n",
        "X_test = test_data[features]\n",
        "\n",
        "# 对测试数据进行预测\n",
        "print(\"对测试数据进行预测...\")\n",
        "y_test_pred = best_rfc.predict(X_test)\n",
        "\n",
        "# 将预测结果转换回原始标签\n",
        "test_data['Status'] = status_le.inverse_transform(y_test_pred)\n",
        "\n",
        "# 将预测结果保存为规定格式的csv文件\n",
        "test_data[['Status']].to_csv('prediction_results.csv', index=False)\n",
        "print(\"预测结果已保存至 prediction_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "开始读取CSV文件...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "读取CSV文件: 100%|██████████| 22/22 [00:01<00:00, 16.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "成功读取22个CSV文件，正在合并数据...\n",
            "数据合并完成，共1034931行，36列。\n",
            "开始处理缺失值...\n",
            "缺失值处理完成，删除了2499行，剩余1032432行。\n",
            "Status列的唯一值： ['normal' 'delay' 'cancel']\n",
            "开始编码分类变量...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "编码分类变量: 100%|██████████| 5/5 [00:00<00:00, 16.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "拆分训练集和验证集...\n",
            "训练集大小：825945，验证集大小：206487\n",
            "开始模型训练和调优...\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "python(24625) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(24626) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(24627) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(24628) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(24629) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(24630) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(24631) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(24632) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(24634) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(24635) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=0.298 total time= 1.3min\n",
            "[CV 3/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=0.298 total time= 1.3min\n",
            "[CV 5/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=0.298 total time= 1.3min\n",
            "[CV 4/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=0.298 total time= 1.3min\n",
            "[CV 2/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=0.298 total time= 1.3min\n",
            "[CV 1/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=0.298 total time= 2.6min\n",
            "[CV 3/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=0.298 total time= 2.6min\n",
            "[CV 2/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=0.298 total time= 1.3min\n",
            "[CV 1/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=0.298 total time= 1.3min\n",
            "[CV 5/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=0.298 total time= 1.3min\n",
            "[CV 3/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=0.298 total time= 1.3min\n",
            "[CV 4/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=0.298 total time= 2.6min\n",
            "[CV 4/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=0.298 total time= 1.3min\n",
            "[CV 2/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=0.298 total time= 2.6min\n",
            "[CV 5/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=0.298 total time= 2.6min\n",
            "[CV 1/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=0.320 total time= 2.2min\n",
            "[CV 4/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=0.320 total time= 2.2min\n",
            "[CV 5/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=0.319 total time= 2.2min\n",
            "[CV 2/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=0.319 total time= 2.2min\n",
            "[CV 3/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=0.319 total time= 2.2min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "python(25514) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(25520) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(25522) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "python(25523) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=0.298 total time= 2.5min\n",
            "[CV 4/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=0.298 total time= 2.5min\n",
            "[CV 5/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=0.298 total time= 2.5min\n",
            "[CV 2/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=0.298 total time= 2.6min\n",
            "[CV 3/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=0.298 total time= 2.6min\n",
            "[CV 1/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=0.318 total time= 2.2min\n",
            "[CV 2/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=0.317 total time= 2.2min\n",
            "[CV 3/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=0.318 total time= 2.2min\n",
            "[CV 4/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=0.317 total time= 2.2min\n",
            "[CV 5/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=0.317 total time= 2.2min\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 102\u001b[0m\n\u001b[1;32m     92\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m     93\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mrfc,\n\u001b[1;32m     94\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# 进行网格搜索\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# 输出最佳参数\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m最佳参数：\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAA1001FP/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAA1001FP/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1023\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1018\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1023\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAA1001FP/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1570\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1570\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAA1001FP/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    965\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    966\u001b[0m         )\n\u001b[1;32m    967\u001b[0m     )\n\u001b[0;32m--> 969\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m     )\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAA1001FP/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAA1001FP/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAA1001FP/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAA1001FP/lib/python3.9/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 导入必要的库\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 设置学生信息\n",
        "student_name = 'Your Name'\n",
        "student_id = 'Your Student ID'\n",
        "\n",
        "# 1. 读取并合并2016-2017年的航班数据\n",
        "csv_files = glob.glob('flights-data/*.csv')\n",
        "data_list = []\n",
        "print(\"开始读取CSV文件...\")\n",
        "for file in tqdm(csv_files, desc=\"读取CSV文件\"):\n",
        "    df = pd.read_csv(file)\n",
        "    data_list.append(df)\n",
        "print(f\"成功读取{len(csv_files)}个CSV文件，正在合并数据...\")\n",
        "data = pd.concat(data_list, ignore_index=True)\n",
        "print(f\"数据合并完成，共{data.shape[0]}行，{data.shape[1]}列。\")\n",
        "\n",
        "# 2. 数据清洗和预处理\n",
        "print(\"开始处理缺失值...\")\n",
        "initial_rows = data.shape[0]\n",
        "data.dropna(inplace=True)\n",
        "removed_rows = initial_rows - data.shape[0]\n",
        "print(f\"缺失值处理完成，删除了{removed_rows}行，剩余{data.shape[0]}行。\")\n",
        "\n",
        "# 确认Status列的取值\n",
        "print(\"Status列的唯一值：\", data['Status'].unique())\n",
        "\n",
        "# 将Status列编码为数字\n",
        "status_le = LabelEncoder()\n",
        "data['Status'] = status_le.fit_transform(data['Status'])\n",
        "\n",
        "# 3. 编码分类变量\n",
        "print(\"开始编码分类变量...\")\n",
        "categorical_features = ['UniqueCarrier', 'Origin', 'Dest', 'DepTimeBlk', 'ArrTimeBlk']\n",
        "label_encoders = {}\n",
        "for col in tqdm(categorical_features, desc=\"编码分类变量\"):\n",
        "    le = LabelEncoder()\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# 4. 处理时间特征\n",
        "# CRSDepTime和CRSArrTime转换为分钟数\n",
        "def convert_time(x):\n",
        "    x = int(x)\n",
        "    if x == 2400:\n",
        "        x = 0\n",
        "    hours = x // 100\n",
        "    minutes = x % 100\n",
        "    return hours * 60 + minutes\n",
        "\n",
        "data['CRSDepTime'] = data['CRSDepTime'].apply(convert_time)\n",
        "data['CRSArrTime'] = data['CRSArrTime'].apply(convert_time)\n",
        "\n",
        "# 5. 选择特征和目标变量\n",
        "features = ['Quarter', 'Month', 'DayofMonth', 'DayOfWeek',\n",
        "            'CRSDepTime', 'CRSArrTime', 'UniqueCarrier', 'Origin', 'Dest',\n",
        "            'DepTimeBlk', 'ArrTimeBlk', 'Distance', 'DistanceGroup']\n",
        "\n",
        "X = data[features]\n",
        "y = data['Status']\n",
        "\n",
        "# 6. 拆分训练集和验证集\n",
        "print(\"拆分训练集和验证集...\")\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "print(f\"训练集大小：{X_train.shape[0]}，验证集大小：{X_valid.shape[0]}\")\n",
        "\n",
        "# 7. 使用随机森林进行模型训练和调优\n",
        "print(\"开始模型训练和调优...\")\n",
        "\n",
        "# 定义参数网格\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "# 初始化随机森林分类器\n",
        "rfc = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 初始化 GridSearchCV，设置 verbose=3 以获取详细输出\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rfc,\n",
        "    param_grid=param_grid,\n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "    scoring='f1_macro',\n",
        "    n_jobs=-1,  # 使用所有可用的CPU核心\n",
        "    verbose=3\n",
        ")\n",
        "\n",
        "# 进行网格搜索\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 输出最佳参数\n",
        "print(\"最佳参数：\", grid_search.best_params_)\n",
        "\n",
        "# 在验证集上预测\n",
        "print(\"在验证集上进行预测...\")\n",
        "best_rfc = grid_search.best_estimator_\n",
        "y_pred = best_rfc.predict(X_valid)\n",
        "\n",
        "# 输出F1分数和准确率\n",
        "f1 = f1_score(y_valid, y_pred, average='macro')\n",
        "acc = accuracy_score(y_valid, y_pred)\n",
        "print(f\"验证集 F1 分数: {f1:.4f}\")\n",
        "print(f\"验证集 准确率: {acc:.4f}\")\n",
        "\n",
        "# 混淆矩阵和分类报告\n",
        "print(\"分类报告：\")\n",
        "print(classification_report(y_valid, y_pred, target_names=status_le.classes_))\n",
        "\n",
        "# 8. 特征重要性分析\n",
        "importances = best_rfc.feature_importances_\n",
        "feature_importance = pd.Series(importances, index=features).sort_values(ascending=False)\n",
        "print(\"特征重要性：\")\n",
        "print(feature_importance)\n",
        "\n",
        "# 可视化特征重要性\n",
        "plt.figure(figsize=(10,6))\n",
        "feature_importance.plot(kind='bar')\n",
        "plt.title('Feature Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 9. 对测试数据进行同样的预处理，并进行预测\n",
        "print(\"开始处理测试数据并进行预测...\")\n",
        "test_data = pd.read_csv('test_data.csv')\n",
        "print(f\"测试数据共{test_data.shape[0]}行，{test_data.shape[1]}列。\")\n",
        "test_data.dropna(inplace=True)\n",
        "print(f\"处理缺失值后，测试数据剩余{test_data.shape[0]}行。\")\n",
        "\n",
        "# 编码测试数据的分类变量\n",
        "for col in categorical_features:\n",
        "    le = label_encoders[col]\n",
        "    test_data[col] = le.transform(test_data[col])\n",
        "\n",
        "# 处理时间特征\n",
        "test_data['CRSDepTime'] = test_data['CRSDepTime'].apply(convert_time)\n",
        "test_data['CRSArrTime'] = test_data['CRSArrTime'].apply(convert_time)\n",
        "\n",
        "# 提取特征\n",
        "X_test = test_data[features]\n",
        "\n",
        "# 对测试数据进行预测\n",
        "print(\"对测试数据进行预测...\")\n",
        "y_test_pred = best_rfc.predict(X_test)\n",
        "\n",
        "# 将预测结果转换回原始标签\n",
        "test_data['Status'] = status_le.inverse_transform(y_test_pred)\n",
        "\n",
        "# 将预测结果保存为规定格式的csv文件\n",
        "test_data[['Status']].to_csv('prediction_results.csv', index=False)\n",
        "print(\"预测结果已保存至 prediction_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "读取CSV文件: 100%|██████████| 22/22 [00:01<00:00, 16.07it/s]\n",
            "编码分类变量: 100%|██████████| 6/6 [00:00<00:00, 15.66it/s]\n",
            "[I 2024-12-18 15:54:28,298] A new study created in RDB with name: lightgbm_f1_optimization\n",
            "[I 2024-12-18 15:55:58,727] Trial 5 finished with value: 0.29805591742113247 and parameters: {'learning_rate': 0.0012080838994751929, 'num_leaves': 129, 'max_depth': 5, 'min_child_samples': 83, 'subsample': 0.7428959954998875, 'colsample_bytree': 0.6954751184670124, 'reg_alpha': 0.00015503101038965013, 'reg_lambda': 0.06352619468795004}. Best is trial 5 with value: 0.29805591742113247.\n",
            "[I 2024-12-18 15:56:02,210] Trial 3 finished with value: 0.29805591742113247 and parameters: {'learning_rate': 0.007001980240422246, 'num_leaves': 160, 'max_depth': 5, 'min_child_samples': 51, 'subsample': 0.8190883558506318, 'colsample_bytree': 0.531384115194945, 'reg_alpha': 0.10669206394352647, 'reg_lambda': 7.573636652659845}. Best is trial 3 with value: 0.29805591742113247.\n",
            "[I 2024-12-18 15:56:46,088] Trial 1 finished with value: 0.29805591742113247 and parameters: {'learning_rate': 0.007036897798300271, 'num_leaves': 254, 'max_depth': 6, 'min_child_samples': 8, 'subsample': 0.5823103288631319, 'colsample_bytree': 0.8735497048985219, 'reg_alpha': 0.001351077690747401, 'reg_lambda': 0.0001351432097906513}. Best is trial 1 with value: 0.29805591742113247.\n",
            "[I 2024-12-18 15:58:13,856] Trial 4 finished with value: 0.29805591742113247 and parameters: {'learning_rate': 0.001177558821807593, 'num_leaves': 84, 'max_depth': 10, 'min_child_samples': 12, 'subsample': 0.866685284014884, 'colsample_bytree': 0.6152097892313955, 'reg_alpha': 0.24304450195468824, 'reg_lambda': 0.003968120444139629}. Best is trial 1 with value: 0.29805591742113247.\n",
            "[I 2024-12-18 15:59:04,138] Trial 12 finished with value: 0.29805591742113247 and parameters: {'learning_rate': 0.007214424317545389, 'num_leaves': 169, 'max_depth': 6, 'min_child_samples': 49, 'subsample': 0.7325200496994788, 'colsample_bytree': 0.8449898314273192, 'reg_alpha': 0.005102648110029673, 'reg_lambda': 0.11663658720646732}. Best is trial 1 with value: 0.29805591742113247.\n",
            "[I 2024-12-18 15:59:41,943] Trial 10 finished with value: 0.29805591742113247 and parameters: {'learning_rate': 0.011452711893872303, 'num_leaves': 75, 'max_depth': 12, 'min_child_samples': 23, 'subsample': 0.9313138721780992, 'colsample_bytree': 0.5885060275819249, 'reg_alpha': 0.0034032488604688726, 'reg_lambda': 0.5529822673375632}. Best is trial 1 with value: 0.29805591742113247.\n",
            "[I 2024-12-18 16:00:06,399] Trial 9 finished with value: 0.29870153060536486 and parameters: {'learning_rate': 0.014394249385409688, 'num_leaves': 239, 'max_depth': 8, 'min_child_samples': 17, 'subsample': 0.716934639597148, 'colsample_bytree': 0.7295879201494679, 'reg_alpha': 8.32361248746201, 'reg_lambda': 0.007202134705497173}. Best is trial 9 with value: 0.29870153060536486.\n",
            "[I 2024-12-18 16:00:14,355] Trial 11 finished with value: 0.29805591742113247 and parameters: {'learning_rate': 0.00628643659427404, 'num_leaves': 97, 'max_depth': 12, 'min_child_samples': 13, 'subsample': 0.9807750384555979, 'colsample_bytree': 0.761822808538697, 'reg_alpha': 0.5755506790652687, 'reg_lambda': 0.1625677023456794}. Best is trial 9 with value: 0.29870153060536486.\n",
            "[I 2024-12-18 16:00:45,877] Trial 8 finished with value: 0.31379729535727435 and parameters: {'learning_rate': 0.03324698677186598, 'num_leaves': 178, 'max_depth': 10, 'min_child_samples': 71, 'subsample': 0.635760755633299, 'colsample_bytree': 0.928514334326195, 'reg_alpha': 4.926794343877281, 'reg_lambda': 0.0006098416871426944}. Best is trial 8 with value: 0.31379729535727435.\n",
            "[I 2024-12-18 16:01:16,996] Trial 13 finished with value: 0.3051947901549123 and parameters: {'learning_rate': 0.02920537209934303, 'num_leaves': 63, 'max_depth': 12, 'min_child_samples': 43, 'subsample': 0.7293244375056718, 'colsample_bytree': 0.8378402920398484, 'reg_alpha': 0.0006075847873729102, 'reg_lambda': 2.4206759855736486}. Best is trial 8 with value: 0.31379729535727435.\n",
            "[I 2024-12-18 16:01:22,747] Trial 0 finished with value: 0.29805591742113247 and parameters: {'learning_rate': 0.003889916706745986, 'num_leaves': 199, 'max_depth': 10, 'min_child_samples': 14, 'subsample': 0.7560597874978716, 'colsample_bytree': 0.78026886726718, 'reg_alpha': 0.0001899336721023169, 'reg_lambda': 0.010724386956492053}. Best is trial 8 with value: 0.31379729535727435.\n"
          ]
        }
      ],
      "source": [
        "# 导入必要的库\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import multiprocessing\n",
        "\n",
        "student_id = \"50012962\"\n",
        "student_name = \"Bowen_LIU\"\n",
        "\n",
        "# 读取并合并数据\n",
        "csv_files = glob.glob('flights-data/*.csv')\n",
        "data_list = []\n",
        "for file in tqdm(csv_files, desc=\"读取CSV文件\"):\n",
        "    df = pd.read_csv(file)\n",
        "    data_list.append(df)\n",
        "data = pd.concat(data_list, ignore_index=True)\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# 编码分类变量\n",
        "categorical_features = ['UniqueCarrier', 'Origin', 'Dest', 'DepTimeBlk', 'ArrTimeBlk', 'Status']\n",
        "label_encoders = {}\n",
        "for col in tqdm(categorical_features, desc=\"编码分类变量\"):\n",
        "    le = LabelEncoder()\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# 处理时间特征\n",
        "def convert_time(x):\n",
        "    try:\n",
        "        x = int(x)\n",
        "    except:\n",
        "        x = 0\n",
        "    if x == 2400:\n",
        "        x = 0\n",
        "    hours = x // 100\n",
        "    minutes = x % 100\n",
        "    return hours * 60 + minutes\n",
        "\n",
        "data['CRSDepTime'] = data['CRSDepTime'].apply(convert_time)\n",
        "data['CRSArrTime'] = data['CRSArrTime'].apply(convert_time)\n",
        "\n",
        "# 选择特征和目标变量\n",
        "features = ['Month', 'DayofMonth', 'DayOfWeek',\n",
        "            'CRSDepTime', 'CRSArrTime', 'UniqueCarrier', 'Origin', 'Dest',\n",
        "            'DepTimeBlk', 'ArrTimeBlk', 'Distance']\n",
        "X = data[features]\n",
        "y = data['Status']\n",
        "\n",
        "# 拆分训练集和验证集\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# 定义Optuna的目标函数\n",
        "# 定义Optuna的目标函数\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': len(np.unique(y_train)),\n",
        "        'metric': 'multi_logloss',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 31, 256),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 10.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 10.0, log=True),\n",
        "    }\n",
        "    \n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    f1_scores = []\n",
        "    accuracies = []\n",
        "    \n",
        "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "        \n",
        "        model = lgb.LGBMClassifier(**param)\n",
        "        model.fit(X_tr, y_tr)\n",
        "        \n",
        "        y_pred = model.predict(X_val)\n",
        "        \n",
        "        f1 = f1_score(y_val, y_pred, average='macro')\n",
        "        acc = accuracy_score(y_val, y_pred)\n",
        "        \n",
        "        f1_scores.append(f1)\n",
        "        accuracies.append(acc)\n",
        "    \n",
        "    mean_f1 = np.mean(f1_scores)\n",
        "    mean_acc = np.mean(accuracies)\n",
        "    \n",
        "    # 将 F1 分数和准确率保存为用户属性，便于在 optuna-dashboard 中查看\n",
        "    trial.set_user_attr('f1_score', mean_f1)\n",
        "    trial.set_user_attr('accuracy', mean_acc)\n",
        "    \n",
        "    # 返回平均 F1 分数（最大化）\n",
        "    return mean_f1\n",
        "\n",
        "# 创建Optuna的study并优化，设置优化方向为最大化\n",
        "study = optuna.create_study(\n",
        "    direction='maximize',\n",
        "    sampler=TPESampler(seed=42),\n",
        "    storage='sqlite:///optuna_study.db',\n",
        "    study_name='lightgbm_f1_optimization',\n",
        "    load_if_exists=True\n",
        ")\n",
        "\n",
        "# 使用多进程并行优化\n",
        "if __name__ == '__main__':\n",
        "    n_trials = 50\n",
        "    n_jobs = multiprocessing.cpu_count()\n",
        "    study.optimize(objective, n_trials=n_trials, n_jobs=n_jobs)\n",
        "\n",
        "# 输出最佳参数\n",
        "print(\"最佳参数：\", study.best_params)\n",
        "\n",
        "# 使用最佳参数训练最终模型\n",
        "best_params = study.best_params\n",
        "best_params['objective'] = 'multiclass'\n",
        "best_params['num_class'] = len(np.unique(y_train))\n",
        "best_params['metric'] = 'multi_logloss'\n",
        "best_params['verbosity'] = -1\n",
        "best_params['boosting_type'] = 'gbdt'\n",
        "\n",
        "model = lgb.LGBMClassifier(**best_params)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 在验证集上进行预测\n",
        "y_pred = model.predict(X_valid)\n",
        "\n",
        "# 计算F1分数和准确率\n",
        "f1 = f1_score(y_valid, y_pred, average='macro')\n",
        "acc = accuracy_score(y_valid, y_pred)\n",
        "print(f\"验证集 F1 分数: {f1:.4f}\")\n",
        "print(f\"验证集 准确率: {acc:.4f}\")\n",
        "\n",
        "# 使用optuna-dashboard可视化优化过程\n",
        "# 在命令行中运行以下命令启动dashboard（需提前安装optuna-dashboard库）\n",
        "# optuna-dashboard sqlite:///optuna_study.db\n",
        "\n",
        "# 处理测试数据并进行预测\n",
        "print(\"开始处理测试数据并进行预测...\")\n",
        "test_data = pd.read_csv('test_data.csv')\n",
        "\n",
        "# 编码测试数据的分类变量\n",
        "for col in categorical_features:\n",
        "    if col != 'Status':  # 测试数据中没有 'Status' 列\n",
        "        le = label_encoders[col]\n",
        "        test_data[col] = le.transform(test_data[col])\n",
        "\n",
        "# 处理时间特征\n",
        "test_data['CRSDepTime'] = test_data['CRSDepTime'].apply(convert_time)\n",
        "test_data['CRSArrTime'] = test_data['CRSArrTime'].apply(convert_time)\n",
        "\n",
        "# 提取特征\n",
        "X_test = test_data[features]\n",
        "\n",
        "# 对测试数据进行预测\n",
        "print(\"对测试数据进行预测...\")\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# 将预测结果转换回原始标签并添加为新列\n",
        "test_data['Status'] = label_encoders['Status'].inverse_transform(y_test_pred)\n",
        "\n",
        "# 保存预测结果\n",
        "test_data[['Status']].to_csv(f\"{student_id}_{student_name}.csv\", index=False)\n",
        "print(f\"预测结果已保存至 {student_id}_{student_name}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "读取CSV文件: 100%|██████████| 22/22 [00:01<00:00, 16.71it/s]\n",
            "编码分类变量: 100%|██████████| 6/6 [00:00<00:00, 17.98it/s]\n",
            "[I 2024-12-18 15:37:12,239] A new study created in RDB with name: random_forest_f1macro_optimization\n",
            "[W 2024-12-18 15:39:49,564] Trial 0 failed with parameters: {'n_estimators': 437, 'max_depth': 48, 'min_samples_split': 8, 'min_samples_leaf': 6, 'bootstrap': True} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"/var/folders/ky/cqbmc7gj63z4z812w3fwdrf80000gp/T/ipykernel_47714/1323954868.py\", line 77, in objective\n",
            "    clf.fit(X_tr, y_tr)\n",
            "  File \"/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 487, in fit\n",
            "    trees = Parallel(\n",
            "  File \"/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
            "    return super().__call__(iterable_with_config)\n",
            "  File \"/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/joblib/parallel.py\", line 2007, in __call__\n",
            "    return output if self.return_generator else list(output)\n",
            "  File \"/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/joblib/parallel.py\", line 1650, in _get_outputs\n",
            "    yield from self._retrieve()\n",
            "  File \"/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/joblib/parallel.py\", line 1762, in _retrieve\n",
            "    time.sleep(0.01)\n",
            "KeyboardInterrupt\n",
            "[W 2024-12-18 15:39:49,586] Trial 0 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 92\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# 8. 创建Optuna的study并优化，使用RDB存储\u001b[39;00m\n\u001b[1;32m     85\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[1;32m     86\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     87\u001b[0m     sampler\u001b[38;5;241m=\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m     load_if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     91\u001b[0m )\n\u001b[0;32m---> 92\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# 输出最佳参数\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m最佳参数：\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
            "Cell \u001b[0;32mIn[2], line 77\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     74\u001b[0m X_tr, X_val \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39miloc[train_idx], X_train\u001b[38;5;241m.\u001b[39miloc[val_idx]\n\u001b[1;32m     75\u001b[0m y_tr, y_val \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39miloc[train_idx], y_train\u001b[38;5;241m.\u001b[39miloc[val_idx]\n\u001b[0;32m---> 77\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m     79\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_val, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:487\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    479\u001b[0m ]\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# 导入必要的库\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "student_id = \"50012962\"\n",
        "student_name = \"Bowen_LIU\"\n",
        "\n",
        "# 读取并合并数据\n",
        "csv_files = glob.glob('flights-data/*.csv')\n",
        "data_list = []\n",
        "for file in tqdm(csv_files, desc=\"读取CSV文件\"):\n",
        "    df = pd.read_csv(file)\n",
        "    data_list.append(df)\n",
        "data = pd.concat(data_list, ignore_index=True)\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# 编码分类变量\n",
        "categorical_features = ['UniqueCarrier', 'Origin', 'Dest', 'DepTimeBlk', 'ArrTimeBlk', 'Status']\n",
        "label_encoders = {}\n",
        "for col in tqdm(categorical_features, desc=\"编码分类变量\"):\n",
        "    le = LabelEncoder()\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# 处理时间特征\n",
        "def convert_time(x):\n",
        "    try:\n",
        "        x = int(x)\n",
        "    except:\n",
        "        x = 0\n",
        "    if x == 2400:\n",
        "        x = 0\n",
        "    hours = x // 100\n",
        "    minutes = x % 100\n",
        "    return hours * 60 + minutes\n",
        "\n",
        "data['CRSDepTime'] = data['CRSDepTime'].apply(convert_time)\n",
        "data['CRSArrTime'] = data['CRSArrTime'].apply(convert_time)\n",
        "\n",
        "# 选择特征和目标变量\n",
        "features = ['Month', 'DayofMonth', 'DayOfWeek',\n",
        "            'CRSDepTime', 'CRSArrTime', 'UniqueCarrier', 'Origin', 'Dest',\n",
        "            'DepTimeBlk', 'ArrTimeBlk', 'Distance']\n",
        "X = data[features]\n",
        "y = data['Status']\n",
        "\n",
        "# 拆分训练集和验证集\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# 定义 Optuna 的目标函数\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
        "    }\n",
        "    \n",
        "    clf = RandomForestClassifier(**param, random_state=42, n_jobs=-1)\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    f1_scores = []\n",
        "    \n",
        "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "        \n",
        "        clf.fit(X_tr, y_tr)\n",
        "        y_pred = clf.predict(X_val)\n",
        "        f1 = f1_score(y_val, y_pred, average='macro')\n",
        "        f1_scores.append(f1)\n",
        "        \n",
        "    return np.mean(f1_scores)\n",
        "\n",
        "# 8. 创建Optuna的study并优化，使用RDB存储\n",
        "study = optuna.create_study(\n",
        "    direction='maximize',\n",
        "    sampler=TPESampler(seed=42),\n",
        "    storage='sqlite:///opt_1218_1.db',  # 使用SQLite数据库存储\n",
        "    study_name='random_forest_f1macro_optimization',\n",
        "    load_if_exists=True\n",
        ")\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# 输出最佳参数\n",
        "print(\"最佳参数：\", study.best_params)\n",
        "\n",
        "# 使用最佳参数训练最终模型\n",
        "best_params = study.best_params\n",
        "best_params['random_state'] = 42\n",
        "best_params['n_jobs'] = -1\n",
        "\n",
        "best_rfc = RandomForestClassifier(**best_params)\n",
        "best_rfc.fit(X_train, y_train)\n",
        "\n",
        "# 在验证集上进行预测\n",
        "y_pred = best_rfc.predict(X_valid)\n",
        "\n",
        "# 计算 F1 分数和准确率\n",
        "f1 = f1_score(y_valid, y_pred, average='macro')\n",
        "acc = accuracy_score(y_valid, y_pred)\n",
        "print(f\"验证集 F1 分数: {f1:.4f}\")\n",
        "print(f\"验证集 准确率: {acc:.4f}\")\n",
        "\n",
        "# 处理测试数据并进行预测\n",
        "print(\"开始处理测试数据并进行预测...\")\n",
        "test_data = pd.read_csv('test_data.csv')\n",
        "\n",
        "# 编码测试数据的分类变量\n",
        "for col in categorical_features:\n",
        "    if col != 'Status':  # 测试数据中没有 'Status' 列\n",
        "        le = label_encoders[col]\n",
        "        test_data[col] = le.transform(test_data[col])\n",
        "\n",
        "# 处理时间特征\n",
        "test_data['CRSDepTime'] = test_data['CRSDepTime'].apply(convert_time)\n",
        "test_data['CRSArrTime'] = test_data['CRSArrTime'].apply(convert_time)\n",
        "\n",
        "# 提取特征\n",
        "X_test = test_data[features]\n",
        "\n",
        "# 对测试数据进行预测\n",
        "print(\"对测试数据进行预测...\")\n",
        "y_test_pred = best_rfc.predict(X_test)\n",
        "\n",
        "# 将预测结果转换回原始标签并添加为新列\n",
        "test_data['Status'] = label_encoders['Status'].inverse_transform(y_test_pred)\n",
        "\n",
        "# 保存预测结果\n",
        "test_data[['Status']].to_csv(f\"{student_id}_{student_name}.csv\", index=False)\n",
        "print(f\"预测结果已保存至 {student_id}_{student_name}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lrrVl5h2dO26"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gdown\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# input your student ID and name here\n",
        "student_id = \"50012962\"\n",
        "student_name = \"Bowen_LIU\"\n",
        "\n",
        "if not os.path.exists(\"test_data.csv\"):\n",
        "    data_url = \"https://drive.google.com/file/d/1eLZRSP9zb9KkdeRg-8CGuak3a_xLS6YF/view?usp=drive_link\"\n",
        "    gdown.download(data_url, fuzzy=True)\n",
        "data = pd.read_csv(\"test_data.csv\")\n",
        "\n",
        "# WARNING: Change this line to your prediction code!!! Do not submit this random result!!!\n",
        "data[\"Status\"] = np.random.choice(np.array([\"normal\", \"cancel\", \"delay\"]), size=len(data), replace=True)\n",
        "\n",
        "# Submit this ipynb and the csv file, you will be graded according to the f1 score (macro) and the quality of your code\n",
        "data.to_csv(f\"{student_id}_{student_name}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "读取CSV文件: 100%|██████████| 22/22 [00:01<00:00, 16.15it/s]\n",
            "编码分类变量: 100%|██████████| 6/6 [00:00<00:00, 16.65it/s]\n",
            "[I 2024-12-18 16:01:58,761] A new study created in RDB with name: lightgbm_f1_optimization\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001243 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1243\n",
            "[LightGBM] [Info] Number of data points in the train set: 660756, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score -4.484366\n",
            "[LightGBM] [Info] Start training from score -1.714110\n",
            "[LightGBM] [Info] Start training from score -0.212461\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001806 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1240\n",
            "[LightGBM] [Info] Number of data points in the train set: 660756, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score -4.484500\n",
            "[LightGBM] [Info] Start training from score -1.714101\n",
            "[LightGBM] [Info] Start training from score -0.212461\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [02:12<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[W 2024-12-18 16:04:11,095] Trial 0 failed with parameters: {'learning_rate': 0.005611516415334507, 'n_estimators': 1000, 'num_leaves': 196, 'max_depth': 30, 'min_child_samples': 19, 'subsample': 0.5779972601681014, 'colsample_bytree': 0.5290418060840998, 'reg_alpha': 2.142302175774105, 'reg_lambda': 0.10129197956845731} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"/var/folders/ky/cqbmc7gj63z4z812w3fwdrf80000gp/T/ipykernel_53374/3460865889.py\", line 90, in objective\n",
            "    model.fit(X_tr, y_tr)\n",
            "  File \"/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/lightgbm/sklearn.py\", line 1284, in fit\n",
            "    super().fit(\n",
            "  File \"/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/lightgbm/sklearn.py\", line 955, in fit\n",
            "    self._Booster = train(\n",
            "  File \"/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/lightgbm/engine.py\", line 307, in train\n",
            "    booster.update(fobj=fobj)\n",
            "  File \"/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/lightgbm/basic.py\", line 4136, in update\n",
            "    _LIB.LGBM_BoosterUpdateOneIter(\n",
            "KeyboardInterrupt\n",
            "[W 2024-12-18 16:04:11,097] Trial 0 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    112\u001b[0m     n_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m--> 113\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# 9. 输出最佳参数\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m最佳参数：\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
            "Cell \u001b[0;32mIn[1], line 90\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     83\u001b[0m y_tr, y_val \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39miloc[train_idx], y_train\u001b[38;5;241m.\u001b[39miloc[val_idx]\n\u001b[1;32m     85\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam,\n\u001b[1;32m     87\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     88\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     89\u001b[0m )\n\u001b[0;32m---> 90\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m     93\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_val, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/lightgbm/sklearn.py:1284\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1282\u001b[0m             valid_sets\u001b[38;5;241m.\u001b[39mappend((valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y)))\n\u001b[0;32m-> 1284\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/lightgbm/sklearn.py:955\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    952\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    953\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_best_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mbest_iteration\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/lightgbm/engine.py:307\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    296\u001b[0m     cb(\n\u001b[1;32m    297\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[1;32m    298\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m         )\n\u001b[1;32m    305\u001b[0m     )\n\u001b[0;32m--> 307\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DSAAnew/lib/python3.9/site-packages/lightgbm/basic.py:4136\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   4134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4135\u001b[0m _safe_call(\n\u001b[0;32m-> 4136\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4137\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4140\u001b[0m )\n\u001b[1;32m   4141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   4142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 1. 导入必要的库\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "import multiprocessing\n",
        "\n",
        "student_id = \"50012962\"\n",
        "student_name = \"Bowen_LIU\"\n",
        "\n",
        "# 2. 读取并合并数据\n",
        "csv_files = glob.glob('flights-data/*.csv')\n",
        "data_list = []\n",
        "for file in tqdm(csv_files, desc=\"读取CSV文件\"):\n",
        "    df = pd.read_csv(file)\n",
        "    data_list.append(df)\n",
        "data = pd.concat(data_list, ignore_index=True)\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# 3. 编码分类变量\n",
        "categorical_features = ['UniqueCarrier', 'Origin', 'Dest', 'DepTimeBlk', 'ArrTimeBlk', 'Status']\n",
        "label_encoders = {}\n",
        "for col in tqdm(categorical_features, desc=\"编码分类变量\"):\n",
        "    le = LabelEncoder()\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# 4. 处理时间特征\n",
        "def convert_time(x):\n",
        "    try:\n",
        "        x = int(x)\n",
        "    except:\n",
        "        x = 0\n",
        "    if x == 2400:\n",
        "        x = 0\n",
        "    hours = x // 100\n",
        "    minutes = x % 100\n",
        "    return hours * 60 + minutes\n",
        "\n",
        "data['CRSDepTime'] = data['CRSDepTime'].apply(convert_time)\n",
        "data['CRSArrTime'] = data['CRSArrTime'].apply(convert_time)\n",
        "\n",
        "# 5. 选择特征和目标变量\n",
        "features = ['Month', 'DayofMonth', 'DayOfWeek',\n",
        "            'CRSDepTime', 'CRSArrTime', 'UniqueCarrier', 'Origin', 'Dest',\n",
        "            'DepTimeBlk', 'ArrTimeBlk', 'Distance']\n",
        "X = data[features]\n",
        "y = data['Status']\n",
        "\n",
        "# 6. 拆分训练集和验证集\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# 7. 定义Optuna的目标函数\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': len(np.unique(y_train)),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 31, 256),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 50, step=5),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 10.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 10.0, log=True),\n",
        "    }\n",
        "    \n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    f1_scores = []\n",
        "    accuracies = []\n",
        "    \n",
        "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "        \n",
        "        model = lgb.LGBMClassifier(\n",
        "            **param,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        model.fit(X_tr, y_tr)\n",
        "        y_pred = model.predict(X_val)\n",
        "        \n",
        "        f1 = f1_score(y_val, y_pred, average='macro')\n",
        "        acc = accuracy_score(y_val, y_pred)\n",
        "        \n",
        "        f1_scores.append(f1)\n",
        "        accuracies.append(acc)\n",
        "    \n",
        "    trial.set_user_attr(\"accuracy\", np.mean(accuracies))\n",
        "    return np.mean(f1_scores)\n",
        "\n",
        "# 8. 创建Optuna的study并优化\n",
        "study = optuna.create_study(\n",
        "    direction='maximize',\n",
        "    sampler=TPESampler(seed=42),\n",
        "    study_name='lightgbm_f1_optimization',\n",
        "    storage='sqlite:///optuna_lgbm_study.db',\n",
        "    load_if_exists=True\n",
        ")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    n_trials = 100\n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "\n",
        "# 9. 输出最佳参数\n",
        "print(\"最佳参数：\", study.best_params)\n",
        "\n",
        "# 10. 使用最佳参数训练最终模型\n",
        "best_params = study.best_params\n",
        "best_params['objective'] = 'multiclass'\n",
        "best_params['num_class'] = len(np.unique(y_train))\n",
        "best_params['random_state'] = 42\n",
        "best_params['n_jobs'] = -1\n",
        "\n",
        "model = lgb.LGBMClassifier(**best_params)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 11. 在验证集上进行预测\n",
        "y_pred = model.predict(X_valid)\n",
        "\n",
        "# 12. 计算F1分数和准确率\n",
        "f1 = f1_score(y_valid, y_pred, average='macro')\n",
        "acc = accuracy_score(y_valid, y_pred)\n",
        "print(f\"验证集 F1 分数: {f1:.4f}\")\n",
        "print(f\"验证集 准确率: {acc:.4f}\")\n",
        "\n",
        "# 13. 使用optuna-dashboard可视化优化过程\n",
        "# 在命令行中运行以下命令启动dashboard（需提前安装optuna-dashboard库）\n",
        "# optuna-dashboard sqlite:///optuna_lgbm_study.db\n",
        "\n",
        "# 14. 处理测试数据并进行预测\n",
        "print(\"开始处理测试数据并进行预测...\")\n",
        "test_data = pd.read_csv('test_data.csv')\n",
        "\n",
        "# 编码测试数据的分类变量\n",
        "for col in categorical_features:\n",
        "    if col != 'Status':  # 测试数据中没有 'Status' 列\n",
        "        le = label_encoders[col]\n",
        "        test_data[col] = le.transform(test_data[col])\n",
        "\n",
        "# 处理时间特征\n",
        "test_data['CRSDepTime'] = test_data['CRSDepTime'].apply(convert_time)\n",
        "test_data['CRSArrTime'] = test_data['CRSArrTime'].apply(convert_time)\n",
        "\n",
        "# 提取特征\n",
        "X_test = test_data[features]\n",
        "\n",
        "# 对测试数据进行预测\n",
        "print(\"对测试数据进行预测...\")\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# 将预测结果转换回原始标签并添加为新列\n",
        "test_data['Status'] = label_encoders['Status'].inverse_transform(y_test_pred)\n",
        "\n",
        "# 保存预测结果\n",
        "test_data[['Status']].to_csv(f\"{student_id}_{student_name}.csv\", index=False)\n",
        "print(f\"预测结果已保存至 {student_id}_{student_name}.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DSAAnew",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
