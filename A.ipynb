{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan distance between P1 and P2: 9\n",
      "Euclidean distance between P2 and P3: 5.0\n",
      "Cosine similarity between P1 and P3: 0.5665288228870652\n",
      "Normalized data objects:\n",
      "[[ 1  0]\n",
      " [ 0  1]\n",
      " [-1  0]\n",
      " [ 0  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Define the data objects\n",
    "p1 = np.array([7, 3])\n",
    "p2 = np.array([4, 9])\n",
    "p3 = np.array([1, 5])\n",
    "p4 = np.array([3, 2])\n",
    "\n",
    "# (1) Calculate the Manhattan distance between P1 and P2\n",
    "md_p1_p2 = np.sum(np.abs(p1 - p2))\n",
    "print(f\"Manhattan distance between P1 and P2: {md_p1_p2}\")\n",
    "\n",
    "# (2) Calculate the Euclidean distance between P2 and P3\n",
    "ed_p2_p3 = np.linalg.norm(p2 - p3)\n",
    "print(f\"Euclidean distance between P2 and P3: {ed_p2_p3}\")\n",
    "\n",
    "# (3) Calculate the Cosine similarity between P1 and P3\n",
    "cs_p1_p3 = 1 - distance.cosine(p1, p3)\n",
    "print(f\"Cosine similarity between P1 and P3: {cs_p1_p3}\")\n",
    "\n",
    "# (4) Normalize the data objects\n",
    "d = np.array([p1, p2, p3, p4])\n",
    "\n",
    "# Normalize the first attribute by Z-score technique\n",
    "m_fa = np.mean(d[:, 0])\n",
    "s_fa = np.std(d[:, 0])\n",
    "d[:, 0] = (d[:, 0] - m_fa) / s_fa\n",
    "\n",
    "# Normalize the second attribute by Min-max technique to interval [0, 1]\n",
    "mi_sa = np.min(d[:, 1])\n",
    "ma_sa = np.max(d[:, 1])\n",
    "d[:, 1] = (d[:, 1] - mi_sa) / (ma_sa - mi_sa)\n",
    "\n",
    "print(\"Normalized data objects:\")\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root node Gini impurity: 0.4922\n",
      "X=0 Gini impurity: 0.4688\n",
      "X=1 Gini impurity: 0.5000\n",
      "X=2 Gini impurity: 0.1528\n",
      "Weighted Gini impurity for attribute X: 0.4028\n",
      "Y=0 Gini impurity: 0.4965\n",
      "Y=1 Gini impurity: 0.4861\n",
      "Weighted Gini impurity for attribute Y: 0.4913\n",
      "Information Gain IG(X): 0.0894\n",
      "Information Gain IG(Y): 0.0009\n",
      "Best split attribute: X\n",
      "\n",
      "Decision Tree Structure:\n",
      "Root node: X\n",
      "├── X=0: Y\n",
      "│   ├── Y=0: cat\n",
      "│   └── Y=1: cat\n",
      "├── X=1: Y\n",
      "│   ├── Y=0: cat\n",
      "│   └── Y=1: dog\n",
      "└── X=2: dog\n",
      "\n",
      "Confusion Matrix:\n",
      "TP (True Positives) = 100\n",
      "FP (False Positives) = 20\n",
      "FN (False Negatives) = 35\n",
      "TN (True Negatives) = 85\n",
      "\n",
      "Accuracy: 0.7708\n",
      "Precision: 0.8333\n",
      "Recall: 0.7407\n",
      "F1-Score: 0.7843\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Build the dataset\n",
    "d = {\n",
    "    'X': [0, 0, 2, 2, 0, 1, 1, 1, 1],\n",
    "    'Y': [1, 1, 0, 0, 0, 0, 0, 1, 1],\n",
    "    'Z': [1, 0, 1, 2, 0, 1, 0, 1, 0],\n",
    "    'dog': [10, 15, 30, 25, 5, 5, 0, 25, 20],\n",
    "    'cat': [20, 15, 5, 0, 15, 20, 15, 10, 5]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "df['total'] = df['dog'] + df['cat']\n",
    "\n",
    "# Calculate the Gini impurity of the root node\n",
    "td = df['dog'].sum()\n",
    "tc = df['cat'].sum()\n",
    "t = td + tc\n",
    "pd_ = td / t\n",
    "pc_ = tc / t\n",
    "gr = 1 - (pd_**2 + pc_**2)\n",
    "print(f\"Root node Gini impurity: {gr:.4f}\")\n",
    "\n",
    "# Function to calculate Gini impurity\n",
    "def gi(dog, cat):\n",
    "    t = dog + cat\n",
    "    if t == 0:\n",
    "        return 0\n",
    "    pd_ = dog / t\n",
    "    pc_ = cat / t\n",
    "    return 1 - (pd_**2 + pc_**2)\n",
    "\n",
    "# (2) Split by attribute X and calculate collective Gini impurity of child nodes\n",
    "gx = df.groupby('X').sum()\n",
    "gx_g = 0\n",
    "for i, r in gx.iterrows():\n",
    "    g = gi(r['dog'], r['cat'])\n",
    "    gx_g += (r['total'] / t) * g\n",
    "    print(f\"X={i} Gini impurity: {g:.4f}\")\n",
    "print(f\"Weighted Gini impurity for attribute X: {gx_g:.4f}\")\n",
    "\n",
    "# (3) Split by attribute Y and calculate collective Gini impurity of child nodes\n",
    "gy = df.groupby('Y').sum()\n",
    "gy_g = 0\n",
    "for i, r in gy.iterrows():\n",
    "    g = gi(r['dog'], r['cat'])\n",
    "    gy_g += (r['total'] / t) * g\n",
    "    print(f\"Y={i} Gini impurity: {g:.4f}\")\n",
    "print(f\"Weighted Gini impurity for attribute Y: {gy_g:.4f}\")\n",
    "\n",
    "# (4) Calculate Information Gain and choose the best split attribute\n",
    "igx = gr - gx_g\n",
    "igy = gr - gy_g\n",
    "print(f\"Information Gain IG(X): {igx:.4f}\")\n",
    "print(f\"Information Gain IG(Y): {igy:.4f}\")\n",
    "bs = 'X' if igx > igy else 'Y'\n",
    "print(f\"Best split attribute: {bs}\")\n",
    "\n",
    "# (5) Build a two-level decision tree and mark class labels in each leaf node\n",
    "print(\"\\nDecision Tree Structure:\")\n",
    "print(\"Root node: X\")\n",
    "print(\"├── X=0: Y\")\n",
    "print(\"│   ├── Y=0: cat\")\n",
    "print(\"│   └── Y=1: cat\")\n",
    "print(\"├── X=1: Y\")\n",
    "print(\"│   ├── Y=0: cat\")\n",
    "print(\"│   └── Y=1: dog\")\n",
    "print(\"└── X=2: dog\")\n",
    "\n",
    "# (6) Calculate the confusion matrix and evaluation metrics for the \"dog\" class\n",
    "# Sample prediction based on the decision tree\n",
    "def p(r):\n",
    "    if r['X'] == 0:\n",
    "        return 'cat'\n",
    "    elif r['X'] == 1:\n",
    "        if r['Y'] == 1:\n",
    "            return 'dog'\n",
    "        else:\n",
    "            return 'cat'\n",
    "    elif r['X'] == 2:\n",
    "        return 'dog'\n",
    "\n",
    "df['pred'] = df.apply(p, axis=1)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tp = df[(df['pred'] == 'dog') & (df['dog'] > 0)]['dog'].sum()\n",
    "fp = df[(df['pred'] == 'dog') & (df['cat'] > 0)]['cat'].sum()\n",
    "fn = df[(df['pred'] == 'cat') & (df['dog'] > 0)]['dog'].sum()\n",
    "tn = df[(df['pred'] == 'cat') & (df['cat'] > 0)]['cat'].sum()\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"TP (True Positives) = {tp}\")\n",
    "print(f\"FP (False Positives) = {fp}\")\n",
    "print(f\"FN (False Negatives) = {fn}\")\n",
    "print(f\"TN (True Negatives) = {tn}\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "acc = (tp + tn) / t\n",
    "prec = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "rec = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "f1 = 2 * prec * rec / (prec + rec) if (prec + rec) != 0 else 0\n",
    "\n",
    "print(f\"\\nAccuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSAA1001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
