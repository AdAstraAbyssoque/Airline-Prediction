{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from xgboost.callback import TrainingCallback\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "categorical_features = ['UniqueCarrier', 'TailNum', 'Status']\n",
    "label_encoders = {}\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def preprocess_data(file_pattern, include_status=True):\n",
    "  csv_files = glob.glob(file_pattern)\n",
    "  data_list = [pd.read_csv(file) for file in csv_files]\n",
    "  if not data_list:\n",
    "    raise ValueError(f\"No files found for pattern {file_pattern}\")\n",
    "  data = pd.concat(data_list, ignore_index=True)\n",
    "  \n",
    "  columns_to_keep = [\n",
    "    'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', 'UniqueCarrier',\n",
    "    'TailNum', 'FlightNum', 'OriginAirportID', 'OriginCityMarketID',\n",
    "    'OriginStateFips', 'OriginWac', 'DestAirportID', 'DestCityMarketID',\n",
    "    'DestStateFips', 'DestWac', 'CRSDepTime', 'CRSArrTime', 'Flights',\n",
    "    'Distance']\n",
    "  if include_status:\n",
    "    columns_to_keep.append('Status')\n",
    "  data = data[columns_to_keep]\n",
    "  \n",
    "  for feature in [f for f in categorical_features if f in data.columns]:\n",
    "    if feature not in label_encoders:\n",
    "      label_encoders[feature] = LabelEncoder()\n",
    "      data[feature] = label_encoders[feature].fit_transform(data[feature])\n",
    "    else:\n",
    "      data[feature] = label_encoders[feature].transform(data[feature])\n",
    "  \n",
    "  data['FlightNum'] = pd.to_numeric(data['FlightNum'], errors='coerce')\n",
    "  data = data.dropna()\n",
    "  \n",
    "  features_to_scale = [col for col in data.columns if col != 'Status']\n",
    "  data[features_to_scale] = scaler.fit_transform(data[features_to_scale])\n",
    "  \n",
    "  return data\n",
    "\n",
    "# Process training data\n",
    "print(\"Start reading training data...\")\n",
    "train_data = preprocess_data('flights-data/*.csv')\n",
    "X = train_data.drop('Status', axis=1)\n",
    "y = train_data['Status']\n",
    "print(f\"Final processed training data size: {train_data.shape}\")\n",
    "\n",
    "# Process test data\n",
    "test_data = preprocess_data('test-data/*.csv', include_status=False)\n",
    "X_test = test_data.copy()\n",
    "print(f\"Final processed test data size: {test_data.shape}\")\n",
    "\n",
    "# Train model using cross-validation\n",
    "print(\"\\nStart training model...\")\n",
    "\n",
    "class TqdmCallback(TrainingCallback):\n",
    "  def __init__(self, total):\n",
    "    super().__init__()\n",
    "    self.pbar = tqdm(total=total, desc='Training')\n",
    "  \n",
    "  def after_iteration(self, model, epoch, evals_log):\n",
    "    self.pbar.update(1)\n",
    "    return False\n",
    "  \n",
    "  def after_training(self, model):\n",
    "    self.pbar.close()\n",
    "    return model\n",
    "\n",
    "# Initialize\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=3407)\n",
    "f1_scores = []\n",
    "models = []\n",
    "test_predictions = []\n",
    "\n",
    "# Cross-validation training\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(X, y), 1):\n",
    "  print(f\"\\nFold {fold}:\")\n",
    "  X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "  y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "  \n",
    "  clf = xgb.XGBClassifier(\n",
    "    n_estimators=10000,\n",
    "    max_depth=20,\n",
    "    learning_rate=0.5,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.75,\n",
    "    objective='multi:softmax',\n",
    "    random_state=3407,\n",
    "    n_jobs=-1,\n",
    "    callbacks=[TqdmCallback(total=10000)]\n",
    "  )\n",
    "  \n",
    "  # Train model\n",
    "  clf.fit(X_train, y_train)\n",
    "  models.append(clf)\n",
    "  \n",
    "  # Validation set evaluation\n",
    "  y_valid_pred = clf.predict(X_valid)\n",
    "  f1 = f1_score(y_valid, y_valid_pred, average='macro')\n",
    "  f1_scores.append(f1)\n",
    "  print(f\"Fold {fold} F1 score: {f1:.4f}\")\n",
    "  print(\"Classification report:\")\n",
    "  print(classification_report(y_valid, y_valid_pred, \n",
    "                target_names=label_encoders['Status'].classes_))\n",
    "  \n",
    "  # Test set prediction\n",
    "  test_pred = clf.predict(X_test)\n",
    "  test_predictions.append(test_pred)\n",
    "\n",
    "print(f\"\\nCross-validation average F1 score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"F1 score standard deviation: {np.std(f1_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble prediction\n",
    "print(\"\\nStarting ensemble prediction...\")\n",
    "test_predictions = np.array(test_predictions)\n",
    "y_test_pred = stats.mode(test_predictions, axis=0).mode.flatten()\n",
    "\n",
    "# Save prediction results\n",
    "original_test_data = pd.read_csv('test-data/test_data.csv')\n",
    "original_test_data['Status'] = label_encoders['Status'].inverse_transform(y_test_pred)\n",
    "output_file_with_original = 'test_data_with_predictions.csv'\n",
    "original_test_data.to_csv(output_file_with_original, index=False)\n",
    "print(f\"Prediction results have been inserted into the original test data and saved to {output_file_with_original}\")\n",
    "\n",
    "# Label distribution analysis\n",
    "print(\"\\nLabel distribution analysis...\")\n",
    "train_label_counts = np.bincount(y.to_numpy())\n",
    "test_label_counts = np.bincount(y_test_pred, minlength=len(train_label_counts))\n",
    "\n",
    "print(\"\\nTraining set label distribution:\")\n",
    "for label, count in enumerate(train_label_counts):\n",
    "  label_name = label_encoders['Status'].classes_[label]\n",
    "  percentage = (count / len(y)) * 100\n",
    "  print(f\"Label {label_name}: {count} ({percentage:.2f}%)\")\n",
    "\n",
    "print(\"\\nTest set predicted label distribution:\")\n",
    "for label, count in enumerate(test_label_counts):\n",
    "  label_name = label_encoders['Status'].classes_[label]\n",
    "  percentage = (count / len(y_test_pred)) * 100\n",
    "  print(f\"Label {label_name}: {count} ({percentage:.2f}%)\")\n",
    "\n",
    "# Distribution difference test\n",
    "observed = np.array([train_label_counts, test_label_counts])\n",
    "chi2, p, dof, expected = chi2_contingency(observed)\n",
    "print(f\"\\nDistribution difference test results:\")\n",
    "print(f\"Chi-squared statistic: {chi2:.4f}\")\n",
    "print(f\"p-value: {p:.4f}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "\n",
    "if p > 0.05:\n",
    "  print(\"There is no significant difference in label distribution between the training set and the test set.\")\n",
    "else:\n",
    "  print(\"There is a significant difference in label distribution between the training set and the test set.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSAAnew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
